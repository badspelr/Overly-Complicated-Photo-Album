# Child Sexual Abuse Material (CSAM) Policy

**Effective Date**: October 18, 2025  
**Last Updated**: October 18, 2025  
**Version**: 1.0

---

## 1. Zero Tolerance Statement

**Photo Album has ZERO TOLERANCE for Child Sexual Abuse Material (CSAM) and child exploitation.**

We are committed to:
- Preventing CSAM from being uploaded or shared on our platform
- Detecting and removing CSAM immediately upon discovery
- Reporting CSAM to the National Center for Missing & Exploited Children (NCMEC) and law enforcement
- Cooperating fully with law enforcement investigations
- Protecting children and preventing harm

**Any user who uploads, shares, or attempts to access CSAM will be immediately and permanently banned, and reported to law enforcement.**

---

## 2. Definitions

### 2.1 Child Sexual Abuse Material (CSAM)
Previously known as "child pornography," CSAM includes:
- Visual depictions (photos, videos, drawings) of minors engaged in sexually explicit conduct
- Realistic depictions (digitally created/altered) of minors in sexual situations
- Content depicting sexual abuse or exploitation of children
- Sexually suggestive images of children in provocative poses

**Minor Definition**: Anyone under 18 years of age (or local age of majority, whichever is higher)

### 2.2 Prohibited Content Categories
- **Explicit CSAM**: Direct depiction of sexual acts involving minors
- **Sexualized Content**: Minors in provocative, sexualized poses or situations
- **Grooming Material**: Content used to normalize sexual contact with minors
- **Child Erotica**: Content sexualizing minors without explicit acts
- **Sextortion**: Coerced sexual images of minors
- **Self-Generated CSAM**: Self-produced images by minors under coercion

---

## 3. Detection and Prevention

### 3.1 Automated Detection
We employ multiple detection mechanisms:

#### AI/ML Scanning
- AI models scan uploaded images for potential CSAM indicators
- Machine learning classifiers flag suspicious content for review
- Hash matching against known CSAM databases (e.g., PhotoDNA)

#### Content Fingerprinting
- Digital fingerprints (hashes) of known CSAM are blocked on upload
- Database synchronized with NCMEC and other trusted sources
- Prevents re-upload of previously identified material

#### Behavioral Analysis
- Unusual upload patterns flagged for review
- Multiple accounts from same IP monitored
- Bulk uploads of mixed content reviewed

### 3.2 Human Review
- Flagged content reviewed by trained safety team
- Reviewers have specialized training in CSAM identification
- Privacy-preserving review processes minimize exposure
- Mental health support provided to review team

### 3.3 User Education
- Terms of Conduct clearly prohibit CSAM
- Upload interface includes warnings and reporting information
- Educational resources about child safety

---

## 4. Reporting Mechanisms

### 4.1 Internal Reporting
**If you encounter suspected CSAM on Photo Album**:

1. **DO NOT interact with the content** (don't download, share, or screenshot)
2. **Report immediately** via:
   - Contact form at `/contact/` with subject "CSAM Report"
   - Emergency email: [dedicated CSAM reporting email]
   - Include: User account, content description, timestamp (DO NOT include images)

3. **Our Response Timeline**:
   - Initial review: **Within 1 hour** of report (24/7 monitoring)
   - Content removal: **Immediate** upon confirmation
   - NCMEC report: **Within 24 hours** as required by law
   - Account termination: **Immediate** and permanent

### 4.2 External Reporting
You can also report directly to:

- **NCMEC CyberTipline**: https://www.cybertipline.org or 1-800-THE-LOST
- **FBI**: https://www.fbi.gov/tips (Internet Crime Complaint Center)
- **Local Law Enforcement**: Call 911 for immediate threats

### 4.3 International Reporting
- **Interpol**: https://www.interpol.int/Crimes/Crimes-against-children
- **Internet Watch Foundation (UK)**: https://www.iwf.org.uk
- **INHOPE Hotlines**: https://www.inhope.org (global network)

---

## 5. Our Response Protocol

### 5.1 Immediate Actions (Within 1 Hour)
Upon detection or report of suspected CSAM:

1. **Content Quarantine**
   - Content immediately removed from public view
   - Content isolated in secure, encrypted storage
   - Access restricted to authorized safety team only

2. **User Account Actions**
   - Account immediately suspended (no login access)
   - All content made private/invisible
   - IP address, device info, and metadata logged

3. **Preservation of Evidence**
   - Content preserved in secure format for law enforcement
   - All associated metadata preserved (upload time, IP, device, location)
   - User account information frozen

### 5.2 Investigation (Within 24 Hours)
1. **Content Verification**
   - Trained reviewers confirm CSAM classification
   - Second reviewer validates initial assessment
   - Legal counsel consulted if needed

2. **Scope Analysis**
   - All user's content reviewed for additional violations
   - Related accounts investigated (same IP, patterns)
   - Shared content recipients identified

### 5.3 Law Enforcement Reporting (Within 24 Hours)
**We are legally required to report CSAM to NCMEC**:

1. **NCMEC CyberTipline Report**
   - Content description and classification
   - User account information (name, email, IP, timestamps)
   - Preserved evidence made available to law enforcement
   - Report confirmation number documented

2. **Additional Reporting**
   - Local law enforcement notified for immediate threats
   - FBI notified for multi-jurisdiction cases
   - International authorities notified if foreign users involved

3. **Ongoing Cooperation**
   - Full cooperation with law enforcement investigations
   - Expedited response to subpoenas and legal requests
   - Testimony in court proceedings if required

### 5.4 Account Termination (Permanent)
- Account permanently disabled (no appeal)
- All content deleted after evidence preservation period
- Email/IP/device banned from creating new accounts
- User banned from all future use of Photo Album

---

## 6. Legal Obligations

### 6.1 Mandatory Reporting
Under **18 U.S.C. § 2258A** (United States):
- Electronic service providers **MUST** report apparent CSAM to NCMEC
- Failure to report is a federal crime
- We comply fully with all reporting requirements

### 6.2 Evidence Preservation
Under **18 U.S.C. § 2258A(f)**:
- Content preserved for **90 days** minimum
- Extended preservation upon law enforcement request
- Secure chain of custody maintained

### 6.3 International Compliance
- **EU**: Comply with EU Directive 2011/93/EU on child sexual abuse
- **UK**: Comply with UK Sexual Offences Act 2003
- **Australia**: Comply with Criminal Code Act 1995
- Other jurisdictions: Comply with local laws requiring CSAM reporting

### 6.4 Safe Harbor Protections
- We act in good faith to identify and report CSAM
- Protected from liability for preserving and reporting content
- No obligation to monitor all content proactively (but we do our best)

---

## 7. Age Verification and Safeguards

### 7.1 Account Creation
- Users must be 13+ years old (16+ in some jurisdictions)
- Age verification via birthdate during registration
- Parental consent required for users under 18 (honor system)

### 7.2 Content Upload Safeguards
- Warning messages during upload about prohibited content
- AI pre-screening before content is saved
- Rate limiting to prevent bulk abuse

### 7.3 Sharing Restrictions
- Private-by-default content (no public sharing)
- Invite-only album sharing (explicit authorization required)
- Share link tracking and expiration

---

## 8. Privacy and Confidentiality

### 8.1 Reporter Privacy
- Reporter identity kept confidential
- Reports cannot be traced back to reporter by other users
- Anonymous reporting accepted

### 8.2 Victim Privacy
- Content removed before public exposure when possible
- Metadata about victims not shared except with law enforcement
- Blurring/redaction used when safe to do so

### 8.3 Law Enforcement Requests
- We comply with lawful subpoenas and search warrants
- User notification provided unless prohibited by law
- Transparency reports published annually

---

## 9. False Positives and Appeals

### 9.1 False Positive Minimization
- Multiple review steps to minimize errors
- Trained human reviewers validate AI flags
- Context considered (family photos, medical, artistic)

### 9.2 Legitimate Family Photos
We understand families share innocent photos of children:
- ✅ Birthday parties, holidays, everyday moments
- ✅ Children in swimsuits at beach/pool (non-sexual context)
- ✅ Baby bathtimes, diaper changes (non-sexualized)

**These are protected and acceptable.**

Context matters:
- ❌ Cropping, zooming, or focusing on private areas = suspicious
- ❌ Provocative poses or sexual staging = prohibited
- ❌ Lack of context suggesting innocent purpose = flagged for review

### 9.3 Appeals (Non-CSAM Cases)
If your content was removed in error (legitimate family photo):
- Contact us via `/contact/` with case details
- Explain context and legitimate purpose
- Review by senior safety team member
- Response within 7 days

**Note**: Confirmed CSAM cases are NOT eligible for appeal.

---

## 10. User Responsibilities

### 10.1 What You Can Do
- Report suspected CSAM immediately
- Use strong passwords to prevent account compromise
- Monitor your shared albums for misuse
- Educate family members about online safety

### 10.2 What You Should NOT Do
- Upload, share, or search for CSAM (illegal and harmful)
- Share account access with minors unsupervised
- Attempt to bypass detection systems
- Create accounts for minors under 13

---

## 11. Prevention and Education

### 11.1 Child Safety Resources
We provide links to:
- **Thorn**: https://www.thorn.org (tech to defend children)
- **NCMEC**: https://www.missingkids.org (prevention resources)
- **NetSmartz**: https://www.netsmartz.org (internet safety education)
- **Common Sense Media**: https://www.commonsensemedia.org

### 11.2 Parental Guidance
- Talk to children about online safety
- Monitor children's online activities
- Use parental controls and supervision
- Report concerning behavior to schools and authorities

---

## 12. Employee and Contractor Training

### 12.1 Safety Team Training
- Specialized CSAM identification training (NCMEC certified)
- Legal requirements and reporting protocols
- Trauma-informed review practices
- Mental health support and counseling access

### 12.2 All Staff Training
- Annual child safety training mandatory
- Recognition of grooming and exploitation patterns
- Reporting procedures and escalation paths
- Confidentiality and legal obligations

---

## 13. Technology Partnerships

We partner with trusted organizations:

- **NCMEC**: CyberTipline integration, hash sharing
- **Microsoft PhotoDNA**: Hash-based CSAM detection
- **Google SafeSearch API**: ML-based content classification
- **INHOPE**: International hotline network coordination
- **Tech Coalition**: Industry best practices and collaboration

---

## 14. Transparency and Accountability

### 14.1 Annual Transparency Report
We publish annual reports including:
- Number of CSAM reports received
- Number of accounts terminated for CSAM
- Number of NCMEC CyberTipline reports filed
- False positive rate and improvements made
- Law enforcement cooperation statistics

### 14.2 Independent Audits
- Annual third-party security audits
- Child safety protocol reviews
- Compliance verification with legal requirements

---

## 15. Continuous Improvement

We are committed to ongoing improvement:
- Regular updates to detection algorithms
- Collaboration with child safety organizations
- Adoption of new technologies (e.g., Microsoft PhotoDNA improvements)
- Feedback from law enforcement and safety experts
- Industry best practices and standards

---

## 16. Legal Disclaimer

**This policy is for informational purposes and does not create legal obligations beyond statutory requirements.**

Photo Album makes best efforts to detect and prevent CSAM but cannot guarantee detection of all prohibited content. Users are responsible for their own content and compliance with laws.

---

## 17. Contact for CSAM Reports

**URGENT CSAM REPORTS (24/7 Monitoring)**:
- **Emergency Email**: [dedicated CSAM email - monitored 24/7]
- **Contact Form**: `/contact/` with subject "CSAM Report"
- **Phone (Urgent Threats)**: [emergency phone number if available]

**Response Time**: Within 1 hour, 24/7/365

**Direct Law Enforcement Reporting**:
- **NCMEC CyberTipline**: https://www.cybertipline.org | 1-800-THE-LOST
- **FBI**: https://tips.fbi.gov | 1-800-CALL-FBI
- **Local Police**: 911 (USA) for immediate threats

---

## 18. Policy Updates

This policy may be updated to reflect:
- Changes in law or legal requirements
- New detection technologies
- Industry best practices
- Feedback from law enforcement and safety organizations

**Version History**:
- **v1.0** (October 18, 2025): Initial publication

---

**Photo Album is committed to protecting children and preventing exploitation. We take these responsibilities seriously and will continue to invest in safety, technology, and partnerships to keep children safe.**

**If you see something, say something. Every report matters.**

---

**Related Policies**:
- [Terms of Conduct](TERMS_OF_CONDUCT.md)
- [Privacy Policy](#)
- [Cookie Policy](#) (available at `/cookie-policy/`)
